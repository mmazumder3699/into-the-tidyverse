---
title: "Session 1: Loading data"
author: "Jae-Young Son"
output:
  html_document:
    code_folding: show
    toc: TRUE
    toc_float: TRUE
---

This document was most recently knit on `r Sys.Date()`.

# Introduction

The first step of any data analysis is (somewhat anti-climatically) figuring out how to load your data. This may seem trivial, but it can be surprisingly difficult, and it is critical to understand how this works before you move on to bigger and more interesting topics.

Here's an example of how it can go terribly wrong. According to a [2016 study](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7), about 20% of the reviewed genetics papers contained errors because Microsoft Excel misinterpreted abbreviations like Septin 2 (SEPT2) as *dates* (September 2nd). This resulted in the HUGO Gene Nomenclature Committee *renaming* gene abbreviations so that Excel doesn't keep messing with people's data (and no, [this is not a joke](https://www.msn.com/en-us/news/technology/scientists-rename-genes-because-microsoft-excel-reads-them-as-dates/ar-BB17EorZ).

With this cautionary tale in mind, let's learn something about how data are commonly saved, and then work through some examples of how to load data. Along the way, we're going to pick up some fundamental prerequisite knowledge about programming.

Before we start, you should create a new R script inside the `Sandbox` folder, where you'll write code corresponding to this tutorial. Please make sure to *save it* in that folder (it's fine that you're saving an empty script for now, you'll fill it in as we go).

# Prerequisite knowledge

## Functions and libraries

Think back to your math classes, where you learned about functions. For example, you may have encountered mathematical functions like:  
$f(x) = x \times 3$, such that $f(5) = 15$)  
$f(x, y) = x^{y}$, such that $f(3, 2) = 9$

As these simple examples demonstrate, a `function` is simply something that takes some `input` and transforms that input into some kind of `output`.

Every data operation we do in R is through the use of some function. For example, I might want to use a function to look through a column of text and mark every row that contains a particular keyword. Here, the input is the column of text and keyword, and the output is a marker for whether a column contains the keyword. So, as you can see, computer functions are not restricted to mathematical operations.

A `library` (sometimes referred to as a `package`) is simply a collection of functions. One library might contain a lot of tools for working with text. Another library might contain tools for performing special kinds of statistical analyses. Some libraries are so critical that they are automatically loaded when you open R. For example, you wouldn't be able to use R at all unless you loaded the `base` library. Others are only loaded when you ask them to be loaded. For example, I almost never work with spatial data or maps. So it would be a huge waste of time and computing resources if I loaded in a bunch of map-making libraries every time I opened R.

If it helps, you can think of a library in a pretty literal sense. One reason why you go to a library is to check out reference books. There is knowledge (functions) you want to access, but your mind (computer's memory) has a limited capacity to store the fine-grained details. So, in order to remind yourself of those details, you open up a reference book (load a library and its functions).

Last time, you installed a few libraries that are particularly important. Here's a reminder of what we did (don't re-install these!):

```{r eval = FALSE}
install.packages("tidyverse")
install.packages("here")
install.packages("janitor")
```

We're going to install one new library, which we'll use in just a moment. You should type this into your console, rather than your script.

```{r eval = FALSE}
install.packages("tictoc")
```

In this series, we'll be making heavy use of the `tidyverse`. The tidyverse is not a single library, but a *collection* of libraries that are designed to work well together. When you get good at using R, you'll load the entire tidyverse at once because you'll know how to use all of the individual libraries in conjunction. However, since we're learning how each library works, we'll load only specific libraries related to specific goals.

Here, our goal is to read data, and so we'll focus on using the `readr` library (part of the tidyverse).

## Variables

Once again, think back to your math classes where you've worked with `variables`. If you have some equation like $y = x \div 4$, you can assign the variable $x$ any value and re-compute the value of $y$.

Computer variables work in much the same way. Try *typing* (**NOT** copy/pasting!) the following into your R console. The symbol `<-` is supposed to look like an arrow, and indicates that the variable to the left of the arrow is being assigned the value of the thing to the right of the arrow.

You can save yourself some time by using the keyboard shortcut `alt + -` (i.e., press down the alt-key and then the dash-key). RStudio automatically turns that into the variable assignment arrow.

```{r}
x <- 12
y <- 4
x/y
```

## The structure of data

For the most part, we are going to work with `tabular data`, so named because this kind of data can be represented as a rectangular table. We will also prioritize tabular data that is encoded as `plaintext`, so named because it can be represented entirely using plain text characters (e.g., no fancy formatting like **bolding** or *italics*).

You are probably familiar with the Microsoft Excel formats `.xls` and `.xlsx`, which are commonly used to store tabular data. We will avoid using Excel files whenever possible, for three reasons. First, Excel files are commonly (ab)used to store random things like side calculations and graphs, in addition to tabular data. Second, with Excel files that contain multiple spreadsheets, it can be difficult to anticipate which sheet you want to load, as virtually no other software makes regular use of multi-spreadsheet formats. Third, the Excel format is a `proprietary` format, meaning that Microsoft owns the intellectual rights to the format and exercises complete control over how it works. If Microsoft decided tomorrow that they wanted to completely change how the Excel data format works, and/or demanded that everyone start paying lots of money to continue using this format, we would have no choice but to obey. If I can get on my soapbox for a moment, I believe that is antithetical to the spirit of open science. Practically speaking, there will be times when you need to use proprietary data formats. We will cover these in a later tutorial. But for now, we will focus on the data formats that are most commonly used in science and government.

By far, the most common format we'll use is `.csv`, which stands for Comma Separated Values. The name makes sense when you open up a CSV in a plaintext editor and look at how the data are formatted. In the below picture, you can see the standard tabular view of the data in the Excel window. In the text editor window, you can see the same data, but in its "raw" plaintext format. Each cell of the Excel spreadsheet corresponds to a text value that is separated by a comma - hence, comma separated values.

`r knitr::include_graphics(here::here("Code", "images", "csv_plaintext.png"))`

Any guesses how a `.tsv` (Tab Separated Values) file might be formatted?

## Dataframes and datatypes

Base R represents tabular data in something called a `dataframe`, which is basically just a table. Each column of a dataframe is named for a single **variable** (e.g., height or age), and each row of that column represents a single **observation** of that variable (e.g., *Jae's* height).

The tidyverse uses a different kind of table known as a `tibble`. There are a number of benefits of using tibbles instead of dataframes, but we'll only focus on one application that is particularly important: preventing mishaps related to datatypes. Base R is known as a `weakly-typed` language, which has nothing to do with the vigor with which you write code, but refers to the fact that R by default is not very careful about its datatypes. For example, consider ZIP codes (for those of you outside the U.S., these are postal codes). Brown University is located at the ZIP code `02912`, and it is important to maintain that leading `0` to avoid mail errors. However, to a computer, that looks awfully like the number `2,912` unless told otherwise. So we might ask R to encode ZIP codes as text (also known as `character` data, or as a `string`) so that we preserve that leading zero. However, a weakly-typed language will often perform `implicit conversion`, which can lead to unexpected results. To see this in action, here's an example:

```{r}
# Here's the year 2020, saved as a string
this_year <- "2020"

# We're making sure that the variable is a string, not a number
is.character(this_year)
is.numeric(this_year)

# Now we test whether this string is equal to the number 2,020
this_year == 2020
```

What happened here? Even though you told R that `this_year` was a string, it `dynamically` converted that string back into a number (i.e., when you checked whether the string was equivalent to the number). That is, it took two different and incompatible datatypes, and `coerced` them to be the same datatype. And, worst of all, R did all this without warning you. Recall the cautionary tale we started out with: Excel did the exact same thing by implicitly converting gene names (encoded as the string datatype) into dates (encoded as the datetime datatype). So, a *weakly*-**typed** language is one in which the preservation of data**types** is *weak*, due to implicit conversion.

Because base R is weakly-typed, so are dataframes. The tidyverse, in contrast, is more `strongly-typed`. Once you set a variable's datatype, it stays that way. If the datatype has to be coerced for some reason, tidyverse functions will at least return a warning. For our purposes, that is the primary benefit of using tibbles over dataframes: they make your datatypes more explicit, and are reluctant to change datatypes unless you explicitly ask for it. For these reasons, the `readr` package automatically creates tibbles when reading data.

## Pathing

There's one final prerequisite we need to cover before we can turn our attention (at long last) to reading data. Your data live somewhere. Maybe they live on your hard drive, or maybe they live somewhere on the internet. In order to tell R where your data live, you need to know the `path` that will take you to your data.

This is a familiar idea if you think about how you navigate around the internet. If we were interested in reading data from the Johns Hopkins covid-19 tracker, we would go to the following URL: ` https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv `. This URL tells us a lot already. The data lives on `githubusercontent.com`, in a subdomain called `raw`. You can get to the data by navigating to the user `CSSEGISandData`, and then going through the subfolders `COVID-19`, then `master`, then `csse_covid_19_data`, then `csse_covid_19_time_series`. Finally, once we're in that folder, the data live in a CSV called `time_series_covid19_confirmed_US.csv`.

The same idea can be applied to your computer's hard drive. I have an (older) copy of these data at `/Users/jaeyoungson/GitHub/into-the-tidyverse/Data/time_series_covid19_confirmed_US.csv`. This path tells me that the CSV `time_series_covid19_confirmed_US.csv` lives inside a folder called `Data`, which lives inside `into-the-tidyverse`, which lives inside `GitHub`, which is associated with the user `jaeyoungson`.

You would typically need to type out the full path every time you wanted R to find a file. This gets tiresome, so we're going to be using a neat trick using the `here` library. In short, this library starts with the folder where your code is stored, then checks whether there is a file with the extension `.Rproj`. If there isn't, it goes up to the parent folder and does the same thing over and over until it finds the `.Rproj` file. If you have organized your project files in a sensible way, this makes it extremely convenient to perform pathing. If this explanation makes no sense to you, check out the examples below and see if you can figure it out.

# Reading data

Today's dataset is adapted from [property tax rolls from the city of Providence, RI](https://data.providenceri.gov/Neighborhoods/2020-Property-Tax-Roll/y9h5-fefu).

First things first, we'll load the libraries we need for today's tutorial. Note that we're using the tidyverse libraries `readr` and `tibble`. The `here` library is used for convenient pathing, and you'll note that it gives you a message about where it will start pathing. The `tictoc` library is used to time how long it takes to run various functions.

```{r}
library(readr)
library(tibble)
library(here)
library(tictoc)
```

## Base R

R comes with a pre-loaded function for reading CSV files, appropriately named `read.csv`. Let's go ahead and take this for a spin. First, we need to find where these data live on our hard drive. We'll use the `here()` function to create the full path.

```{r}
here("Data", "PVD_2020_Property_Tax_Roll.csv")
```

That path is nice, but it's just a string. We need to tell R to do something with that string. So we'll use a `pipe` operator (`%>%`) to take that string and feed it to another function. Just so that we aren't printing off the entire table (which is 44,191 rows long), we'll also pipe to the function `head` so that we're only seeing the first few rows.

```{r}
here("Data", "PVD_2020_Property_Tax_Roll.csv") %>%
  read.csv() %>%
  head()
```

Notably, this hasn't actually saved the dataframe to the R environment. So we need to perform variable assignment.

```{r}
base_read <- here("Data", "PVD_2020_Property_Tax_Roll.csv") %>%
  read.csv()
```

Take a look at your `Environment` pane (by default, in the top right corner of RStudio). You should see a new variable under "Data" that says "base_read". If you click on that, you can look at the entire contents of the dataframe.

We'd like to know what the underlying datatypes are. We can do this using `str`, which prints the **str**ucture of the variable. There are a few things to note here. First, note that this is a `data.frame` object. Second, note that the column `ZIP_POSTAL` is encoded as an `integer`, and that the leading zero in the ZIP code got cut off.

```{r}
base_read %>%
  str()
```

Finally, we'll time how long it takes to read in the data using `tictoc`.

```{r}
# Start timer
tic()

# Read in the data using R's default function for reading CSVs
base_read <- here("Data", "PVD_2020_Property_Tax_Roll.csv") %>%
  read.csv()

# Stop timer
toc()
```

## readr

Now let's try out the tidyverse alternative, `readr`. You'll note that the function is named `readr::read_csv()` instead of the default `utils::read.csv()`.

Sidenote 1: If you've already loaded an entire library, you can simply call any function from that library without fanfare. However, there might be a function you want to load from a big library, which you only want to use once. It would be computationally wasteful to load the whole library in (and if there are multiple libraries that have different functions named the same thing, this creates lots of headaches). You can instead access specific functions using the syntax `library_name::function_name`, even without loading the rest of the library. This can also help disambiguate between similarly-named functions (i.e., it's hard to tell `read.csv()` apart from `read_csv()` if you're not looking carefully).

Sidenote 2: This is a stylistic difference that distinguishes old-school R users (useRs, if you will) from the newer generation of useRs. Other languages like Python and Matlab reserve `.` to do things like access attributes of a class (Python), or to index data from a structure (Matlab). It's not really important to understand what those things mean, other than to appreciate that `.` has no reserved purpose in R, and therefore can be used in variable/function names. This is why older useRs favored function names like `utils::read.csv()`, which are easier to type (because you don't ever need to press the shift key). However, in modern data science, you can use R and Python side-by-side even in the same workflow, and this can lead to some ambiguity. Therefore, tidyverse functions (and others that are tidyverse-friendly) use underscores instead of dots.

```{r}
# Start timer
tic()

# Read in the data using R's default function for reading CSVs
tidy_read <- here("Data", "PVD_2020_Property_Tax_Roll.csv") %>%
  read_csv()

# Stop timer
toc()
```

You'll note a few interesting things that just happened. First, unlike `read.csv()`, tidyr's function explicitly tells you what datatype it used to read in each column. Second, `read_csv()` is much, much faster. The difference is negligible for small datasets, but when you're working with massive datasets, the timing really scales. Third, the algorithm used to guess each column's datatype correctly identified `ZIP_POSTAL` as a character/string datatype, rather than an numeric/integer datatype. We can confirm this by looking at the structure of this variable.

```{r}
tidy_read %>%
  str()
```

You'll note additionally that this is encoded as a `tibble` rather than a `data.frame`.

There's one more cool trick that's worth noting here. In this case, `readr` correctly identified the ZIP code as a character, not a number. But what if it made a mistake that you needed to correct? Or, what if (for whatever reason) you wanted the column `plat` to be read as a string, not a number? We can do this really easily by specifying an additional `argument` to `read_csv()`, which explicitly tells the function to override the default datatype and read in these two columns as strings.

```{r}
tidy_read_mod <- here("Data", "PVD_2020_Property_Tax_Roll.csv") %>%
  read_csv(col_types = cols(ZIP_POSTAL = col_character(),
                            plat = col_character()))
```

# Exercises

A general note: these are hard exercises, and you won't necessarily find the answers in this tutorial. Learning to Google (or, if you really care about privacy, [DuckDuckGo](https://duckduckgo.com/)) answers to your coding questions is itself an essential skill. So, be patient with yourself. Struggling through these exercises will help you understand this material at a deeper level.

Exercise 1: Verify that `tidy_read_mod` read the columns `plat` and `ZIP_POSTAL` as strings.

Exercise 2: The Johns Hopkins covid-19 tracker maintains the latest count of confirmed coronavirus cases in the U.S. at the following URL: `https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv`. *Without downloading the file to your hard drive*, read it into R, and assign it to a variable named `covid_usa`.

Exercise 3: In the `Data` folder, there is a dataset listing all of the [bank locations in Louisville, KY](https://catalog.data.gov/dataset/bank-branches-data/resource/769ef747-f5b8-4154-846e-2f347d45a7ce). The file name is `BankBranchesData.txt`. Read this dataset into R and assign it to a variable named `bank_branches`.

