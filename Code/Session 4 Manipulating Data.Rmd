---
title: "Session 4: Manipulating data"
author: "Jae-Young Son"
output:
  html_document:
    code_folding: show
    toc: TRUE
    toc_float: TRUE
---

This document was most recently knit on `r Sys.Date()`.

# Introduction

Let me clue you into one of the dirty secrets of analyzing data: performing statistical tests only constitutes a tiny fraction of the total time you'll spend working with data. Most of the time, you'll instead be trying to `wrangle` your data into the right format so that you can do whatever statistical tests you want (followed closely by model-building and interpreting statistical results - two topics we'll cover in the second half of this workshop).

Just like last time, we'll start by learning some more fundamental prerequisites about programming. Then, you'll be required to review some concepts from past tutorials (and if you find the review challenging, you may need to revisit past tutorials to refresh your memory).

Before we start, you should create a new R script inside the `Sandbox` folder, where you'll write code corresponding to this tutorial. Please make sure to *save it* in that folder (it's fine that you're saving an empty script for now, you'll fill it in as we go).

This is a fairly hefty tutorial! So much so that I'll be taking a week off before writing the tutorial on using `tidyr`. Hopefully, this gives you some time to go through the exercises and get a lot of wrangling practice.

# Prerequite knowledge

## Functions and variables

We learned a little about functions in the last tutorial. Pause for a moment and see whether you can come up with a good working definition of what a function is.

Here's mine: Functions take some inputs (known as arguments), apply some transformations, and then provide you with the output of those transformations. When we're working inside of a programming environment, nearly *everything* that you ask a computer to do is a function. For example, we're used to thinking about mathematical operations like `2+6` as "arithmetic". But under the hood, what is the computer actually doing?

```{r}
2+6
sum(2, 6)
```

As this example illustrates, the `+` operator acts as a shorthand for the function `sum()`. In this example, all of the numbers that we want to add are `arguments` to that function. This might seem like odd notation at first glance, but recall from your high school math class that these kinds of functions are actually pretty common in math. For example, you might've seen functions written out like this:  
$f(x) = x \times 3$, such that $f(5) = 15$)  
$f(x, y) = x^{y}$, such that $f(3, 2) = 9$

But beware! Functions don't always do what you think they'll do. For example, we'd naively expect that `mean(2, 6)` would result in the output `4`.

```{r}
mean(2, 6)
```

Wait, what happened? To find out, we should consult the function documentation. To do this, type `?mean` into the R console (not your script). In fact, do this now and see what you can figure out. Sometimes, the documentation is really helpful and you can figure out exactly how to use a function from reading it. Other times, you might need to look at the `Examples` at the bottom of the documentation to see sample code. And of course, there's always Google (though you should give [DuckDuckGo](https://duckduckgo.com/) a try if you care about personal privacy).

Here are some hints:
1. You'll note that the argument `x` is supposed to be a `vector`. A vector is nothing more than a (one-dimensional) container for data. For example, let's say that I wanted to record the temperature inside my apartment every hour (an increasingly pressing concern now that we are headed into New England winter). I would have, in colloquial terms, a list of temperatures (list is actually a technical term in computer science, but let's not worry about that for now). If I wanted to represent that list of temperatures with my computer, I could do that using a single vector, which *contains* multiple numbers.
2. In the examples, you'll note that there's another function used: `c()`. What does that function do?
3. Still struggling? Try running `mean(c(2, 6))`. Now, explain why that works as expected.

But what's actually going on inside of a function? The best way to understand this is by creating a custom function.

```{r}
custom_mean <- function(this_vector) {
  sum(this_vector) / length(this_vector)
}
```

You can see that this custom function takes one argument, `this_vector`. It then takes the sum of that vector, then divides by the vector length (i.e., the number of `elements` inside the vector; if the vector contains five numbers, it has a length of five).

Now let's `call` our custom function.

```{r}
custom_mean(c(2, 6))

my_vector <- c(2, 6)
custom_mean(my_vector)
```

Ta-dah! We'll touch on one last thing before moving on. You'll note that our custom function took `this_vector` as an argument. But when we called it, we either fed it an unnamed vector `c(2, 6)`, or a vector that we'd assigned to the variable `my_vector`. What gives? Remember that the point of a variable is that it can stand in for anything. In algebra, $x$ is commonly a variable that can take on the value of numbers ($x=9$), or even other variables ($x=3+9y+\frac{z}{2}$). So when we provide an argument to `custom_mean`, the value of that argument gets assigned to `this_vector`.

We can make this idea really explicit by using `named arguments`:

```{r}
custom_mean(this_vector = c(2, 6))

custom_mean(this_vector = my_vector)
```

So you can see that the function is *creating* a new variable `this_vector`, which is assigned the value of the argument. When we call the function with a different argument, the value of `this_vector` changes to reflect whatever that argument is.

## Libraries

We covered libraries last time as well. What is a library? Try defining it in your mind. Now say that definition out loud and see if you like it.

If you feel uncertain, go back to the last tutorial and refresh your memory on what a library is, and what it's for.

Recall that you can always call a function from a library without loading the entire library. For example:

```{r}
readr::read_csv(here::here("Data", "countypres_2000-2016.csv"),
                n_max = 10)
```

In our last tutorial, recall that we used that `library_name::function_name` syntax to visually differentiate between the functions `utils::read.csv` and `readr::read_csv`, which otherwise look very similar.

Of course, if you plan to use many functions from a library, or to repeatedly use the same (few) functions, it's oftentimes worth it to simply call the entire library. Now that you have a good working idea of what libraries are, load the following libraries: `here`, `readr`, and `dplyr`. If you don't remember how to do this, refer back to the last tutorial.

```{r echo=FALSE}
library(here)
library(readr)
library(dplyr)
```

# dplyr

Today, we're going to focus all of our attention on `dplyr` (pronounced dee-plier, which stands for data-pliers, so named because this library allows you to [ply your data](https://duckduckgo.com/?q=pliers)). There are 7 key verbs you need to know, and the vast majority of your data wrangling will revolve around applying these 7 verbs creatively to get the results you want.

1. `filter`: keep only some rows/observations
2. `select`: keep only some columns/variables
3. `mutate`: add a new column/variable
4. `summarise`: reduce your dataset to a summarized version of a particular column/variable
5. `arrange`: reorder your rows/observations
6. `group_by`: perform operations based on some grouping of your data
7. `join`: "merge" multiple dataframes together

## Read in the data

In the `Data` folder, there is a datafile called `time_series_covid19_confirmed_US.csv`. Using `here` and `readr`, load that data and assign it to a variable named `covid`. There are a few ways you could do this, but you get bonus points if your solution uses the pipe operator `%>%`.

```{r echo=FALSE, message=FALSE}
covid <- here("Data", "time_series_covid19_confirmed_US.csv") %>%
  read_csv()
```

This datafile contains confirmed US covid19 cases, and was downloaded from the [Johns Hopkins coronavirus dashboard](https://github.com/CSSEGISandData/COVID-19).

Let's take a quick look at this dataset. The `dplyr::slice` function returns only the first few rows so that you don't have to scroll through thousands of observations.

```{r}
covid %>%
  slice(1:10)
```

## filter

Every row in this dataset represents one county (or equivalent) inside the United States. Depending on our interests, we might not want the full dataset. For example, if I'm an epidemiologist in California, I might want to `filter` the dataset so that I'm only looking at counties inside California.

That is exactly what `dplyr::filter` allows you to do. (For the time being, ignore `dplyr::select`. I'm only using it here so that I don't print off a billion columns again.)

```{r}
covid %>%
  filter(Province_State == "California") %>%
  select(fips = FIPS, county = Admin2, `9/18/20`:`9/24/20`)
```

You'll note that there's some funky syntax going on here. Let's break that down.

`Province_State` is a variable inside the dataframe `covid`, which indexes what state the data come from. So we're asking to filter only the observations inside the variable `Province_State` that match `California`. The syntax `==` is two equal signs, and means *is exactly equal to*. To see how this works, try playing around with some of these examples:

```{r}
"California" == "California"
"California" == "Texas"
"California" == 319
"California" == FALSE
210 == 210
210*2 == 420
TRUE == FALSE
(TRUE == FALSE) == FALSE
```

If that last one gives you a headache, try this instead: `("California" == "Texas) == FALSE`. And if that still makes you want to throw your computer out the window, ask yourself this: is it true that California is not equal to Texas?

`dplyr:filter` keeps only the observations that return `TRUE` when you test a particular `logical condition`. So let's say that we're only interested in counties where there were at least 1,000 cases on 9/14/20. How would we implement this?

```{r}
covid %>%
  filter(Province_State == "California") %>%
  select(fips = FIPS, county = Admin2, `9/18/20`:`9/24/20`) %>%
  filter(`9/24/20` >= 1000)
```

You may recall learning about equalities and inequalities in your grade-school math classes. As a refresher, here are the basic logical operators you need to know:

1. `==`: exactly equal to
2. `!=`: not equal to
3. `>`: greater than
4. `>=`: at least (greater than or equal to)
5. `<`: less than
6. `<=`: at most (less than or equal to)

## select

Using `dplyr::filter`, we were able to selectively keep certain rows/observations. What if we want to selectively keep certain columns/variables? For example, I might not be interested in knowing the full history of confirmed covid cases. I might only be interested in knowing what happened in the span of a select few days. This is where `dplyr::select` comes in. We've already seen an example of this in action, but let's now dissect how it works.

For the sake of keeping this explanation simple, we'll focus our attention on Yolo County in California using `dplyr::filter`. Then we'll pull out only the FIPS code (a unique numerical identifier for each county), the county name, and records of cases between September 18-24. In this particular dataset, we can see that the FIPS code is stored in a column called `FIPS` and that the county name is in `Admin2`.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(FIPS, Admin2, `9/18/20`:`9/24/20`)
```

You might be wondering what's going on with the syntax `` `9/18/20`:`9/24/20` ``. That weird symbol is **not** an apostrophe, but rather a backtick (truly, a horrible name). On a standard US keyboard, you can find this on the same key as the tilde (i.e., the squiggly `~`). In R, they are interpreted as a `literal` when evaluating nonstandard variable names. Typically, you're not allowed to use variable names that start with a number, and dataframe columns are no exception. However, in this case, the data in its original format uses dates as column names, so we need to tell R to interpret `9/18/20` as a literal. Otherwise, R would interpret it as "9 divided by 18 divided by 20". Below, you can see for yourself that this didn't work as expected.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(FIPS, Admin2, 9/18/20:9/24/20)
```

The second thing you might be curious about is what `:` is doing. The syntax `select(from_here:to_there)` tells R to select all of the columns between `from_here` and `to_there`.

You can also use `dplyr::select` to rename your variables on-the-fly! For example, we might be interested in renaming `FIPS` to `fips` so that we can be lazy about typing the variable name. We might also want to rename `Admin2` to `county` so that our variable name better reflects what data is encoded in that column. Below, you can see an example of how to do this.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(fips = FIPS, county = Admin2, `9/18/20`:`9/24/20`)
```

Doing that is a little more code-efficient than first selecting columns, then renaming them, as you can see below.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(FIPS, Admin2, `9/18/20`:`9/24/20`) %>%
  rename(fips = FIPS, county = Admin2)
```

Finally, you can use `dplyr::select` to *get rid* of columns you don't want anymore.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  filter(Admin2 == "Yolo") %>%
  select(state = Province_State, fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  # Actually, we've changed our mind, we don't want the state variable anymore
  # Get rid of it using the minus sign
  select(-state)
```

## mutate

So far, we've "manipulated" data in the sense that we're only keeping certain parts of the overall dataframe that we want to retain. We can either filter certain observations/rows, or we can select certain variables/columns.

Now, let's turn our attention to creating new variables. One of the standard assumptions of linear models (e.g., statistical tests like t-tests, correlations, and regressions) is that your data come from a normal distribution (sort of - we'll discuss this more in the second half of this workshop when we cover statistical tests). However, count data (e.g., the number of covid19 cases) are almost never normally distributed. A common solution to this is to apply some sort of transformation to the counts before analyzing them, like taking the logarithm or square root.

This is where `dplyr::mutate` comes in. The verb mutate is a bit quirky, but it communicates the point that you're transforming (mutating) your data in some way to make it different from when it started. Let's look at an example to illustrate.

```{r}
# Start off with the original dataframe
covid %>%
  # Keep only observations from California
  filter(Province_State == "California") %>%
  # Let's keep only the first 20 counties using dplyr::slice()
  slice(1:20) %>%
  # Keep only a few columns of interest
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  # Now apply a logarithmic transformation using dplyr::mutate()
  mutate(latest_cases_log = log(latest_cases))
```

So you can see that this created a new column named `latest_cases_log`, which took all the values inside `latest_cases` and applied a function (in this case, `log()`) to all of those values. Note that you could also *overwrite* an existing column, depending on how you name it. For example, we can replace `latest_cases` with a version that contains log-transformed counts.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  mutate(latest_cases = log(latest_cases))
```

And, just to show that this is a general phenomenon, let's overwrite the FIPS column instead. So, be careful about using `dplyr::mutate`, as it can lead to overwriting data if you reuse an existing variable name!

```{r}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  mutate(fips = log(latest_cases))
```

Note that `dplyr::mutate` creates a new variable containing as many observations as in the dataframe used to create that variable. You should also know that you can also create a column containing a single value for every single one of those observations. For the sake of demonstration, let's pretend that the covid19 counts are stored in different CSV files according to what state they come from. After opening up the California counts, we would then want to create a column indicating that these particular data come from California. How would we do this?

```{r}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  # Pretend that we're getting this data from a CSV that doesn't contain state info
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  mutate(state = "California")
```

Finally, you might be interesting in applying the same function to multiple columns. For example, you might be interested in applying a log transformation not just to the covid19 counts on 9/24/20, but for that entire week. We could do that using a function called `dplyr::across`. This function takes two arguments: one specifying which columns you want to mutate, and another specifying what you want to do with those columns.

```{r}
covid %>%
  select(state = Province_State, county = Admin2, `9/18/20`:`9/24/20`) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  mutate(across(.cols = `9/18/20`:`9/24/20`,
                .fns = ~log(.x + 1)))
```

This is now 100% a digression, but let's take a second to talk about that second argument, `.fns`. The syntax we're using is kind of funky, and it's not immediately intuitive what we're doing here: `.fns = ~log(.x + 1)`. This is known as a `lambda function`, which is actually pretty advanced and way outside the scope of this workshop. The need-to-know takeaway is that you can create a new function on-the-fly, which is not named (contrast this against the `my_mean` function we built earlier), and only used once. The squiggly `~` (called a tilde) signals that you're creating a lambda function, and the phrase `.x` refers to the data you're passing to the lambda function. In this case, `.x` refers to each of the columns that you want to mutate, and the syntax specifies that we want to take the logarithm of each column after adding 1 to its covid19 count. This is because $log(0) = -\infty$ - to avoid getting this as a result, we add the constant 1 so that when there are zero cases, we get the easier-to-work-with result $log(1) = 0$. Still feel like lambda functions are inscrutable? No worries, this is a level of programming abstraction that's really hard to wrap your mind around. I'll show you one final example of how we'd build a normal function that does the same thing, which might help you make the connection. And if you don't get it, it's not the end of the world.

```{r}
# Our custom function. Note its similarity with the lambda function we used earlier!
avoid_log_trap <- function(x) {
  log(x + 1)
}

covid %>%
  select(state = Province_State, county = Admin2, `9/18/20`:`9/24/20`) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  mutate(across(.cols = `9/18/20`:`9/24/20`,
                # Everything is the same up to this point
                # Now we replace the lambda function with our custom function
                .fns = avoid_log_trap))
```

## summarise

Sometimes, we don't want to compute a value for every observation, but rather a single value *summarizing* over observations. For example, given a vector of numbers, we might be interested in knowing a summary statistic about this vector: its mean, standard deviation, min/max, sum, and so on. This is what the function `dplyr::summarise` is for. (Sidenote: If you're from the US, you're probably more used to the spelling *summarize*. There is a corresponding function called `dplyr::summarize` that literally just calls `dplyr::summarise` under the hood. Either is fine, but pick one and stick to it.)

So for example, we might be interested in knowing the total number of confirmed covid19 cases on a particular day. How would we compute that?

```{r}
covid %>%
  filter(Province_State == "California") %>%
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  summarise(total_cases = sum(latest_cases))
```

And there we have it. You can see that it operates in a way that's very similar to `dplyr::mutate`.

That goes for the use of `dplyr::across` as well. If we're interested in getting the daily sum of all Californian covid19 cases, we can do this pretty easily.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  select(fips = FIPS, county = Admin2, `9/18/20`:`9/24/20`) %>%
  summarise(across(.cols = `9/18/20`:`9/24/20`,
                   .fns = sum))
```

## arrange

It is oftentimes useful to reorder your observations. You can do this easily using `dplyr::arrange`. For example, we might want to reorder our dataframe according to the number of covid19 cases.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  arrange(latest_cases)
```

Similarly, we could arrange the dataframe according to county names. Just for demonstration, we'll do it in reverse alphabetical order using the function `desc`.

```{r}
covid %>%
  filter(Province_State == "California") %>%
  slice(1:5) %>%
  select(fips = FIPS, county = Admin2, latest_cases = `9/24/20`) %>%
  arrange(desc(county))
```

## group_by

Oftentimes, we have dataframes where observations can be grouped together. The `dplyr::group_by` function doesn't do anything on its own, but it's extremely powerful when you use it in conjunction with other functions.

For example, in our original `covid` dataframe, we have data for every county in every state in the US. We could be interested in summarizing the total number of covid19 cases at the state level, which requires grouping together observations from counties that belong to the same state.

```{r}
covid %>%
  rename(state = Province_State, latest_cases = `9/24/20`) %>%
  group_by(state) %>%
  summarise(n_cases = sum(latest_cases)) %>%
  # When you're done manipulating grouped data, remember to ungroup!
  # Otherwise, you may get unexpected (and hard-to-diagnose) problems later on.
  ungroup() %>%
  arrange(desc(n_cases))
```

## join

As data scientists, we want to know not only how many cases there are, but what kinds of factors *predict* why some places have more cases than others. This requires us to incorporate more data into our analyses. For example, it's not particularly surprising that California and Texas have the most number of covid19 cases. Why? Well, they just happen to have the biggest populations in the entire US. We might instead want to know whether, *proportional to their population size*, these states are doing worse than, say, North Dakota. We can't do that without having additional data about each county's population. We might also be interested in understanding whether counties are doing systematically better/worse depending on how urban they are. And finally, we know that Republican-leaning and Democrat-leaning areas have taken different public health approaches to combatting the virus. A natural question we might have is whether certain kinds of public health policies/philosophies are associated with covid19 prevalence. So, if we had an index of a county's political leanings, we could see whether that has a relationship with how well/poorly a given county is managing its covid19 cases.

To answer these questions, we'll need to take various pieces of information stored in different dataframes, and find a way to `join` them all together. So, the final wrangling functions we'll discuss today are all the variants of mutating joins:
1. `dplyr::inner_join`
2. `dplyr::left_join`
3. `dplyr::right_join`
4. `dplyr::full_join`

In order to demonstrate the power of joining, we'll need to pull in a few other datasets. First, let's read in the [CDC NCHS' urban-rural classification scheme](https://www.cdc.gov/nchs/data_access/urban_rural.htm), which classifies every county along a spectrum of urbanicity (from rural to metro). When we take a look at these data, we notice that the `urbanicity` dataset contains a FIPS identifier, just like the `covid` dataset.

```{r message=FALSE}
urbanicity <- here("Data", "NCHSURCodes2013.xlsx") %>%
  readxl::read_excel(na = c(".")) %>%
  janitor::clean_names() %>%
  select(fips_code, urbanicity = x2013_code, population = county_2012_pop)

urbanicity %>%
  slice(1:5)
```

Now, let's pull in county-level voting data from the 2016 election, downloaded from [MIT's Election Lab](https://electionlab.mit.edu/data). In their original form, these data simply give you the total number of votes for Trump, and the total number of votes for Clinton. Our goal is to create a single-number index of how much a particular county voted for Trump over Clinton. One method for doing this is to find the ratio $\frac{votes_{Trump}}{votes_{Clinton}}$. This ratio can be interpreted such that 1 = same number of people voted for Trump and Clinton, and numbers greater than 1 indicate more preference for Trump (e.g., a ratio of 5 would mean five times as many people voted for Trump than Clinton). Take a few minutes to look through the code below, and try to verbalize what each line of code is doing. Then, run each line one-by-one so that you can see what's happening at each processing step.

```{r message=FALSE}
elections <- here("Data", "countypres_2000-2016.csv") %>%
  read_csv() %>%
  filter(year == 2016) %>%
  filter(party %in% c("democrat", "republican")) %>%
  group_by(state, county, FIPS) %>%
  mutate(lean_republican = candidatevotes / first(candidatevotes)) %>%
  ungroup() %>%
  filter(party == "republican") %>%
  select(state, county, FIPS, lean_republican)
```

Let's take a look at this wrangled dataset. Again, there's that old friend of ours, the FIPS code.

```{r}
elections %>%
  slice(1:5)
```

As usual, it's easier to show you how a function works than to describe it. There are few things to note here.

First, even though the `urbanicity` and `elections` datasets contain information about all counties in the US, it only joins information about the 10 counties we kept from `covid`. This is a property of the specific kind of join we performed (`dplyr::left_join`), and we'll later explore how other kinds of joins work.

Second, if there are multiple columns (variables) that are shared between two dataframes, joins will (by default) try to match data together based on *all* of them. You'll note that when you run the line `left_join(elections)`, we got a message saying `Joining, by = c("FIPS", "county", "state")`, which indicates that these three columns were present in both `covid` and `elections`.

Third, even though we had FIPS codes in all three datasets, the column name was different in the `urbanicity` dataset. So when we joined this dataset with the others, we had to manually specify that the column `FIPS` in `covid` and `elections` contained the same data as the column `fips_code` in `elections`. We did this using the argument `by=`, which took a vector (created by `c()`). This vector is a little bit different from the vectors we've seen before, where we stored a simple "list" of elements (e.g., `c(1, 2, 3, 4)`). This is known as a `named vector`, because every element in the vector has a corresponding "name". It might help to think of a named vector as a *dictionary* (e.g., try typing ``c(`1`="one fish", `2`="two fish", `3`="red fish", `4`="blue fish")`` into the console).

```{r}
# Start with the covid count data
covid %>%
  select(FIPS, county = Admin2, state = Province_State, latest_cases = `9/24/20`) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  # Join with the election data
  left_join(elections) %>%
  # Join with the population data
  left_join(urbanicity, by=c("FIPS"="fips_code"))
```

Now, let's do the same thing, but instead of using `dplyr::left_join`, we'll use `dplyr::right_join`. This is key: we're only going to be keeping the first 10 rows of `covid`, but then we'll try joining that 10-row dataset with the full `elections` dataset. You can see that for the first ten rows, the join worked as expected. However, when we look at the next ten rows, there's no data in the `latest_cases` column! This is because `dplyr::left_join` keeps all rows from the `left-hand side (LHS)`, which in this case was 10 rows from `covid`. On the other hand, `dplyr::right_join` keeps all rows from the `right-hand side (RHS)`, which is all 3158 rows of `elections` (we restricted the output to 20 observations at the very end so that you don't have to scroll through 1000s of observations; try running this without the final line to verify that all rows of `elections` ended up in the output dataframe).

In practice, I almost always use left joins instead of right joins. This is because I'm starting with a given dataframe, then iteratively using pipes to manipulate it. So when I'm joining a RHS dataframe to a LHS dataframe that I've already been working with, I typically want to keep all of the data from the LHS dataframe.

```{r}
# Start with the covid count data
covid %>%
  select(FIPS, county = Admin2, state = Province_State, latest_cases = `9/24/20`) %>%
  filter(state == "California") %>%
  slice(1:10) %>%
  # Join with the election data
  right_join(elections) %>%
  # Restrict to first 20 observations so we don't print 1000s of rows
  slice(1:20)
```

What if you only wanted to keep rows where there was a match in the LHS **and** RHS dataframes? To illustrate, let's take a look at a geographical location that has entries in `covid`, but not in `elections`. Since Puerto Ricans are not eligible to vote in US elections, there are no rows in `elections` corresponding to Puerto Rico, as we can see below.

```{r}
elections %>%
  filter(state == "Puerto Rico")
```

What does the left join do when there are rows in LHS (`covid`), but not RHS (`elections`)? Take a second to predict what output you'd expect when joining those dataframes. Now that you've made a prediction, let's see if you're right. For illustration, we'll compare the output for Puerto Rico vs California. Because left joins keep all rows from LHS, the output dataframe keeps those rows and produces `NA`s when there is no corresponding data in RHS.

```{r}
covid %>%
  select(FIPS, county = Admin2, state = Province_State, latest_cases = `9/24/20`) %>%
  filter(state %in% c("Puerto Rico", "California")) %>%
  # Note how the combination of group_by and slice results in an output where we keep
  # the first five rows from *both* Puerto Rico and California!
  group_by(state) %>%
  slice(1:5) %>%
  ungroup() %>%
  left_join(elections)
```

Sometimes, you might only want to keep data when there are matching rows in LHS and RHS. In that case, you'd want to use `dplyr::inner_join`. Let's run the same code as before, but replace `dplyr::left_join` with `dplyr::inner_join`. We can see from this output that only the rows from California were kept.

```{r}
covid %>%
  select(FIPS, county = Admin2, state = Province_State, latest_cases = `9/24/20`) %>%
  filter(state %in% c("Puerto Rico", "California")) %>%
  group_by(state) %>%
  slice(1:5) %>%
  ungroup() %>%
  inner_join(elections)
```

Finally, there are times when you might want to keep **all** rows from LHS and RHS. To avoid printing out the full output, I'll restrict `covid` so that it only contains 5 counties from Puerto Rico and California, and `elections` so that it only contains 5 counties from Alabama. In doing so, I've made it so that there are **no** matching rows between LHS and RHS. We can still join these together and keep all possible rows by using `dplyr::full_join`.

```{r}
covid %>%
  select(FIPS, county = Admin2, state = Province_State, latest_cases = `9/24/20`) %>%
  filter(state %in% c("Puerto Rico", "California")) %>%
  group_by(state) %>%
  slice(1:5) %>%
  ungroup() %>%
  # For the purpose of illustration, restricting elections to 5 Alabama counties
  full_join(elections %>%
              filter(state == "Alabama") %>%
              slice(1:5))
```

# Exercises

A general note: these are hard exercises, and you won't necessarily find the answers in this tutorial. Learning to Google (or, if you really care about privacy, [DuckDuckGo](https://duckduckgo.com/)) answers to your coding questions is itself an essential skill. So, be patient with yourself. Struggling through these exercises will help you understand this material at a deeper level.

The Vera Institute of Justice has a publicly-available dataset on [county-level incarceration trends over time](https://github.com/vera-institute/incarceration-trends). You can find it in `Data/incarceration_trends.csv`. To give you a flavor for what real-world data wrangling is like, you're going to perform lots of different operations with this dataset.

First, read in the data (using `here` and `readr`), and assign it to a variable named `incarceration`.

Create a new dataframe called `ca_jail`, which keeps only the **2018** records from **California**. You can do this in two lines of code, but as a challenge, see if you can do it in one (hint: look up the documentation for `base::Logic`).

For our purposes, there are only a handful of variables we want to keep: the FIPS identifier, each county's total population (`total_pop`), and the total jail population (for this exercise, we won't be looking at prison populations).

It's hard to compare counties' jail populations because every county has a different population size. Find some way to address this problem. A sensible name for the column you create might be `prop_jail` (prop = proportion).

Let's say a colleague of yours is interested in how a county's political leanings affect its tendency to jail people. Luckily, we've just been working with a dataset that can address this. Create a version of `ca_jail` that incorporates relevant data from `elections`. Rearrange the rows of this dataframe (using code!) according to the `lean_republican` ratio. By looking at the first 10 rows and last 10 rows, provide a qualitative answer for your colleague.

We might be able to do better. Create a variable indicating whether a county's `lean_republican` ratio is greater than 1 (meaning that more people voted for Trump than for Clinton). A sensible name for this column might be something like `more_trump`. Based on whether counties had more Trump or Clinton voters in 2016, summarize **both** the mean and standard deviation of `prop_jail`. Two hints may help you accomplish this. Hint 1: your output table should have two rows. Hint 2: the function `mean` doesn't like it when there are `NA`s in the data, and you'll note that FIPS area 6003 has no data about jail population. Take a look at the documentation for this function to see what can be done about this.

Now that you've done this for California, do it for the entire USA. Are the results what you expected? How do you interpret these results?

Finally, following up on that last prompt, it may be instructive to aggregate not at the level of the entire USA, but at the state level. You might find it instructive to read this NPR article entitled ["How Louisiana Became The World's Prison Capital"](https://www.npr.org/2012/06/05/154352977/how-louisiana-became-the-worlds-prison-capital). Does this change your interpretation? Why?

